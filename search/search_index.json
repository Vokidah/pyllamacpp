{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PyLLaMaCpp API Reference","text":""},{"location":"#pyllamacpp.model","title":"pyllamacpp.model","text":"<p>This module contains a simple Python API around llama.cpp</p>"},{"location":"#pyllamacpp.model.Model","title":"Model","text":"<pre><code>Model(ggml_model, log_level=logging.INFO, **llama_params)\n</code></pre> <p>A simple Python class on top of llama.cpp</p> <p>Example usage <pre><code>def new_text_callback(text):\n    print(text, end=\"\")\n\nmodel = Model(ggml_model='./models/ggml-model-f16-q4_0.bin', n_ctx=512)\nmodel.generate(\"hi my name is \", n_predict=55, new_text_callback=new_text_callback)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>ggml_model</code> <code>str</code> <p>the path to the ggml model</p> required <code>log_level</code> <code>int</code> <p>logging level, set to INFO by default</p> <code>logging.INFO</code> <code>llama_params</code> <p>keyword arguments for different whisper.cpp parameters, see PARAMS_SCHEMA</p> <code>{}</code> Source code in <code>pyllamacpp/model.py</code> <pre><code>def __init__(self,\n             ggml_model: str,\n             log_level: int = logging.INFO,\n             **llama_params):\n\"\"\"\n    :param ggml_model: the path to the ggml model\n    :param log_level: logging level, set to INFO by default\n    :param llama_params: keyword arguments for different whisper.cpp parameters,\n                    see [PARAMS_SCHEMA](/pyllamacpp/#pyllamacpp.constants.LLAMA_CONTEXT_PARAMS_SCHEMA)\n    \"\"\"\n    # set logging level\n    set_log_level(log_level)\n    self._ctx = None\n\n    if not Path(ggml_model).is_file():\n        raise Exception(f\"File {ggml_model} not found!\")\n\n    self.llama_params = pp.llama_context_default_params()\n    # update llama_params\n    self._set_params(self.llama_params, llama_params)\n\n    self._ctx = pp.llama_init_from_file(ggml_model, self.llama_params)\n\n    # gpt params\n    self.gpt_params = pp.gpt_params()\n\n    self.res = \"\"\n</code></pre>"},{"location":"#pyllamacpp.model.Model.generate","title":"generate","text":"<pre><code>generate(\n    prompt,\n    n_predict=128,\n    new_text_callback=None,\n    verbose=False,\n    **gpt_params\n)\n</code></pre> <p>Runs llama.cpp inference to generate new text content from the prompt provided as input</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>the prompt</p> required <code>n_predict</code> <code>int</code> <p>number of tokens to generate</p> <code>128</code> <code>new_text_callback</code> <code>Callable[[str], None]</code> <p>a callback function called when new text is generated, default <code>None</code></p> <code>None</code> <code>verbose</code> <code>bool</code> <p>print some info about the inference</p> <code>False</code> <code>gpt_params</code> <p>any other llama.cpp params see PARAMS_SCHEMA</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>the new generated text</p> Source code in <code>pyllamacpp/model.py</code> <pre><code>def generate(self, prompt: str,\n             n_predict: int = 128,\n             new_text_callback: Callable[[str], None] = None,\n             verbose: bool = False,\n             **gpt_params) -&gt; str:\n\"\"\"\n    Runs llama.cpp inference to generate new text content from the prompt provided as input\n\n    :param prompt: the prompt\n    :param n_predict: number of tokens to generate\n    :param new_text_callback: a callback function called when new text is generated, default `None`\n    :param verbose: print some info about the inference\n    :param gpt_params: any other llama.cpp params see [PARAMS_SCHEMA](/pyllamacpp/#pyllamacpp.constants.GPT_PARAMS_SCHEMA)\n    :return: the new generated text\n    \"\"\"\n    self.gpt_params.prompt = prompt\n    self.gpt_params.n_predict = n_predict\n    # update other params if any\n    self._set_params(self.gpt_params, gpt_params)\n\n    # assign new_text_callback\n    self.res = \"\"\n    Model._new_text_callback = new_text_callback\n\n    # run the prediction\n    pp.llama_generate(self._ctx, self.gpt_params, self._call_new_text_callback, verbose)\n    return self.res\n</code></pre>"},{"location":"#pyllamacpp.model.Model.get_params","title":"get_params  <code>staticmethod</code>","text":"<pre><code>get_params(params)\n</code></pre> <p>Returns a <code>dict</code> representation of the params</p> <p>Returns:</p> Type Description <code>dict</code> <p>params dict</p> Source code in <code>pyllamacpp/model.py</code> <pre><code>@staticmethod\ndef get_params(params) -&gt; dict:\n\"\"\"\n    Returns a `dict` representation of the params\n    :return: params dict\n    \"\"\"\n    res = {}\n    for param in dir(params):\n        if param.startswith('__'):\n            continue\n        res[param] = getattr(params, param)\n    return res\n</code></pre>"},{"location":"#pyllamacpp.model.Model.get_params_schema","title":"get_params_schema  <code>staticmethod</code>","text":"<pre><code>get_params_schema()\n</code></pre> <p>A simple link to PARAMS_SCHEMA</p> <p>Returns:</p> Type Description <code>dict</code> <p>dict of params schema</p> Source code in <code>pyllamacpp/model.py</code> <pre><code>@staticmethod\ndef get_params_schema() -&gt; dict:\n\"\"\"\n    A simple link to [PARAMS_SCHEMA](/pyllamacpp/#pyllamacpp.constants.PARAMS_SCHEMA)\n    :return: dict of params schema\n    \"\"\"\n    return constants.GPT_PARAMS_SCHEMA\n</code></pre>"},{"location":"#pyllamacpp.constants","title":"pyllamacpp.constants","text":"<p>Constants</p>"},{"location":"#pyllamacpp.constants.PACKAGE_NAME","title":"PACKAGE_NAME  <code>module-attribute</code>","text":"<pre><code>PACKAGE_NAME = 'pyllamacpp'\n</code></pre>"},{"location":"#pyllamacpp.constants.LOGGING_LEVEL","title":"LOGGING_LEVEL  <code>module-attribute</code>","text":"<pre><code>LOGGING_LEVEL = logging.INFO\n</code></pre>"},{"location":"#pyllamacpp.constants.LLAMA_CONTEXT_PARAMS_SCHEMA","title":"LLAMA_CONTEXT_PARAMS_SCHEMA  <code>module-attribute</code>","text":"<pre><code>LLAMA_CONTEXT_PARAMS_SCHEMA = {\n    \"n_ctx\": {\n        \"type\": int,\n        \"description\": \"text context\",\n        \"options\": None,\n        \"default\": -1,\n    },\n    \"n_parts\": {\n        \"type\": int,\n        \"description\": \"\",\n        \"options\": None,\n        \"default\": -1,\n    },\n    \"seed\": {\n        \"type\": int,\n        \"description\": \"RNG seed, 0 for random\",\n        \"options\": None,\n        \"default\": 0,\n    },\n    \"f16_kv\": {\n        \"type\": bool,\n        \"description\": \"use fp16 for KV cache\",\n        \"options\": None,\n        \"default\": 0,\n    },\n    \"logits_all\": {\n        \"type\": bool,\n        \"description\": \"the llama_eval() call computes all logits, not just the last one\",\n        \"options\": None,\n        \"default\": 0,\n    },\n    \"vocab_only\": {\n        \"type\": bool,\n        \"description\": \"only load the vocabulary, no weights\",\n        \"options\": None,\n        \"default\": 0,\n    },\n    \"use_mlock\": {\n        \"type\": bool,\n        \"description\": \"force system to keep model in RAM\",\n        \"options\": None,\n        \"default\": 0,\n    },\n    \"embedding\": {\n        \"type\": bool,\n        \"description\": \"embedding mode only\",\n        \"options\": None,\n        \"default\": 0,\n    },\n}\n</code></pre>"},{"location":"#pyllamacpp.constants.GPT_PARAMS_SCHEMA","title":"GPT_PARAMS_SCHEMA  <code>module-attribute</code>","text":"<pre><code>GPT_PARAMS_SCHEMA = {\n    \"seed\": {\n        \"type\": int,\n        \"description\": \"RNG seed\",\n        \"options\": None,\n        \"default\": -1,\n    },\n    \"n_predict\": {\n        \"type\": int,\n        \"description\": \"Number of tokens to predict\",\n        \"options\": None,\n        \"default\": 50,\n    },\n    \"n_threads\": {\n        \"type\": int,\n        \"description\": \"Number of threads\",\n        \"options\": None,\n        \"default\": 4,\n    },\n    \"repeat_last_n\": {\n        \"type\": int,\n        \"description\": \"Last n tokens to penalize\",\n        \"options\": None,\n        \"default\": 64,\n    },\n    \"top_k\": {\n        \"type\": int,\n        \"description\": \"top_k\",\n        \"options\": None,\n        \"default\": 40,\n    },\n    \"top_p\": {\n        \"type\": float,\n        \"description\": \"top_p\",\n        \"options\": None,\n        \"default\": 0.95,\n    },\n    \"temp\": {\n        \"type\": float,\n        \"description\": \"temp\",\n        \"options\": None,\n        \"default\": 0.8,\n    },\n    \"repeat_penalty\": {\n        \"type\": float,\n        \"description\": \"repeat_penalty\",\n        \"options\": None,\n        \"default\": 1.3,\n    },\n    \"n_batch\": {\n        \"type\": int,\n        \"description\": \"batch size for prompt processing\",\n        \"options\": None,\n        \"default\": True,\n    },\n}\n</code></pre>"},{"location":"#pyllamacpp.utils","title":"pyllamacpp.utils","text":"<p>Helper functions</p>"},{"location":"#pyllamacpp.utils.llama_to_ggml","title":"llama_to_ggml","text":"<pre><code>llama_to_ggml(dir_model, ftype=1)\n</code></pre> <p>A helper function to convert LLaMa Pytorch models to ggml, same exact script as <code>convert-pth-to-ggml.py</code> from llama.cpp repository, copied here for convinience purposes only!</p> <p>Parameters:</p> Name Type Description Default <code>dir_model</code> <code>str</code> <p>llama model directory</p> required <code>ftype</code> <code>int</code> <p>0 or 1, 0-&gt; f32, 1-&gt; f16</p> <code>1</code> <p>Returns:</p> Type Description <code>str</code> <p>ggml model path</p> Source code in <code>pyllamacpp/utils.py</code> <pre><code>def llama_to_ggml(dir_model: str, ftype: int = 1) -&gt; str:\n\"\"\"\n    A helper function to convert LLaMa Pytorch models to ggml,\n    same exact script as `convert-pth-to-ggml.py` from [llama.cpp](https://github.com/ggerganov/llama.cpp)\n    repository, copied here for convinience purposes only!\n\n    :param dir_model: llama model directory\n    :param ftype: 0 or 1, 0-&gt; f32, 1-&gt; f16\n    :return: ggml model path\n    \"\"\"\n    # output in the same directory as the model\n    assert ftype in [0, 1], f\"ftype should be in [0,1], 0-&gt; f32, 1-&gt; f16\"\n\n    fname_hparams = str((Path(dir_model) / \"params.json\").absolute())\n    fname_tokenizer = str((Path(dir_model).parent / \"tokenizer.model\").absolute())\n\n    def get_n_parts(dim):\n        if dim == 4096:\n            return 1\n        elif dim == 5120:\n            return 2\n        elif dim == 6656:\n            return 4\n        elif dim == 8192:\n            return 8\n        else:\n            print(\"Invalid dim: \" + str(dim))\n            sys.exit(1)\n\n    # possible data types\n    #   ftype == 0 -&gt; float32\n    #   ftype == 1 -&gt; float16\n    #\n    # map from ftype to string\n    ftype_str = [\"f32\", \"f16\"]\n\n    with open(fname_hparams, \"r\") as f:\n        hparams = json.load(f)\n\n    tokenizer = SentencePieceProcessor(fname_tokenizer)\n\n    hparams.update({\"vocab_size\": tokenizer.vocab_size()})\n\n    n_parts = get_n_parts(hparams[\"dim\"])\n\n    print(hparams)\n    print('n_parts = ', n_parts)\n\n    for p in range(n_parts):\n        print('Processing part ', p)\n\n        # fname_model = dir_model + \"/consolidated.00.pth\"\n\n        fname_model = str(Path(dir_model) / f\"consolidated.0{str(p)}.pth\")\n        fname_out = str(Path(dir_model) / f\"ggml-model-{ftype_str[ftype]}.bin\")\n        if (p &gt; 0):\n            fname_out = str(Path(dir_model) / f\"ggml-model-{ftype_str[ftype]}.bin.{str(p)}\")\n\n        model = torch.load(fname_model, map_location=\"cpu\")\n\n        fout = open(fname_out, \"wb\")\n\n        fout.write(struct.pack(\"i\", 0x67676d6c))  # magic: ggml in hex\n        fout.write(struct.pack(\"i\", hparams[\"vocab_size\"]))\n        fout.write(struct.pack(\"i\", hparams[\"dim\"]))\n        fout.write(struct.pack(\"i\", hparams[\"multiple_of\"]))\n        fout.write(struct.pack(\"i\", hparams[\"n_heads\"]))\n        fout.write(struct.pack(\"i\", hparams[\"n_layers\"]))\n        fout.write(struct.pack(\"i\", hparams[\"dim\"] // hparams[\"n_heads\"]))  # rot (obsolete)\n        fout.write(struct.pack(\"i\", ftype))\n\n        # Is this correct??\n        for i in range(32000):\n            if tokenizer.is_unknown(i):\n                # \"&lt;unk&gt;\" token (translated as ??)\n                text = \" \\u2047 \".encode(\"utf-8\")\n                fout.write(struct.pack(\"i\", len(text)))\n                fout.write(text)\n            elif tokenizer.is_control(i):\n                # \"&lt;s&gt;\"/\"&lt;/s&gt;\" tokens\n                fout.write(struct.pack(\"i\", 0))\n            elif tokenizer.is_byte(i):\n                # \"&lt;U+XX&gt;\" tokens (which may be invalid UTF-8)\n                piece = tokenizer.id_to_piece(i)\n                if len(piece) != 6:\n                    print(\"Invalid token: \" + piece)\n                    sys.exit(1)\n                byte_value = int(piece[3:-1], 16)\n                fout.write(struct.pack(\"i\", 1))\n                fout.write(struct.pack(\"B\", byte_value))\n            else:\n                # normal token. Uses U+2581 (LOWER ONE EIGHTH BLOCK) to represent spaces.\n                text = tokenizer.id_to_piece(i).replace(\"\\u2581\", \" \").encode(\"utf-8\")\n                fout.write(struct.pack(\"i\", len(text)))\n                fout.write(text)\n\n        for k, v in model.items():\n            name = k\n            shape = v.shape\n\n            # skip layers.X.attention.inner_attention.rope.freqs\n            if name[-5:] == \"freqs\":\n                continue\n\n            print(\"Processing variable: \" + name + \" with shape: \", shape, \" and type: \", v.dtype)\n\n            # data = tf.train.load_variable(dir_model, name).squeeze()\n            data = v.numpy().squeeze()\n            n_dims = len(data.shape);\n\n            # for efficiency - transpose some matrices\n            # \"model/h.*/attn/c_attn/w\"\n            # \"model/h.*/attn/c_proj/w\"\n            # \"model/h.*/mlp/c_fc/w\"\n            # \"model/h.*/mlp/c_proj/w\"\n            # if name[-14:] == \"/attn/c_attn/w\" or \\\n            #   name[-14:] == \"/attn/c_proj/w\" or \\\n            #   name[-11:] == \"/mlp/c_fc/w\" or \\\n            #   name[-13:] == \"/mlp/c_proj/w\":\n            #    print(\"  Transposing\")\n            #    data = data.transpose()\n\n            dshape = data.shape\n\n            # default type is fp16\n            ftype_cur = 1\n            if ftype == 0 or n_dims == 1:\n                print(\"  Converting to float32\")\n                data = data.astype(np.float32)\n                ftype_cur = 0\n\n            # header\n            sname = name.encode('utf-8')\n            fout.write(struct.pack(\"iii\", n_dims, len(sname), ftype_cur))\n            for i in range(n_dims):\n                fout.write(struct.pack(\"i\", dshape[n_dims - 1 - i]))\n            fout.write(sname);\n\n            # data\n            data.tofile(fout)\n\n        # I hope this deallocates the memory ..\n        model = None\n\n        fout.close()\n\n        print(\"Done. Output file: \" + fname_out + \", (part \", p, \")\")\n        print(\"\")\n        return fname_out\n</code></pre>"},{"location":"#pyllamacpp.utils.quantize","title":"quantize","text":"<pre><code>quantize(ggml_model_path, output_model_path=None, itype=2)\n</code></pre> <p>Qunatizes the ggml model.</p> <p>Parameters:</p> Name Type Description Default <code>ggml_model_path</code> <code>str</code> <p>path of the ggml model</p> required <code>output_model_path</code> <code>str</code> <p>output file path for the qunatized model</p> <code>None</code> <code>itype</code> <code>int</code> <p>quantization type: 2 -&gt; Q4_0, 3 -&gt; Q4_1</p> <code>2</code> <p>Returns:</p> Type Description <code>str</code> <p>quantized model path</p> Source code in <code>pyllamacpp/utils.py</code> <pre><code>def quantize(ggml_model_path: str, output_model_path: str = None, itype: int = 2) -&gt; str:\n\"\"\"\n    Qunatizes the ggml model.\n\n    :param ggml_model_path: path of the ggml model\n    :param output_model_path: output file path for the qunatized model\n    :param itype: quantization type: 2 -&gt; Q4_0, 3 -&gt; Q4_1\n    :return: quantized model path\n    \"\"\"\n    if output_model_path is None:\n        output_model_path = ggml_model_path + f'-q4_{0 if itype == 2 else 1}.bin'\n    logging.info(\"Quantization will start soon ... (This my take a while)\")\n    pp.llama_quantize(ggml_model_path, output_model_path, itype)\n    logging.info(f\"Quantized model is created successfully {output_model_path}\")\n    return output_model_path\n</code></pre>"}]}